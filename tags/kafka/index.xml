<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kafka on Me &amp; Web</title>
    <link>http://lovelock.coding.me/tags/kafka/</link>
    <description>Recent content in Kafka on Me &amp; Web</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>(c) 2013-2016 Frost Wong. All rights reserved.</copyright>
    <lastBuildDate>Wed, 11 Jan 2017 14:27:46 +0800</lastBuildDate>
    <atom:link href="/tags/kafka/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>清理HDFS的脏数据</title>
      <link>http://lovelock.coding.me/bigdata/cleanup-dirty-data-in-hdfs/</link>
      <pubDate>Wed, 11 Jan 2017 14:27:46 +0800</pubDate>
      
      <guid>http://lovelock.coding.me/bigdata/cleanup-dirty-data-in-hdfs/</guid>
      <description>

&lt;p&gt;周末同事在群里用由我维护的Hive里查数据，发现完全对不上了，我简单查了一下，发现是预计下一期上的日志格式提前上了，而我的Topology还没有跟上，导致数据字段完全错乱。我马上停掉相应的Topology，避免产生更多的脏数据，周一到公司在给新版的Topology做了严格测试准备上线。&lt;/p&gt;

&lt;p&gt;接下来面对的就是怎么把脏数据清除掉，把被错误处理的数据重新处理一遍，并且后续的数据不受影响。&lt;/p&gt;

&lt;h2 id=&#34;步骤&#34;&gt;步骤&lt;/h2&gt;

&lt;h3 id=&#34;1-找到脏数据的offset-最好向前移动一些-以保证所有脏数据都能被处理&#34;&gt;1. 找到脏数据的offset，最好向前移动一些，以保证所有脏数据都能被处理&lt;/h3&gt;

&lt;p&gt;在数据处理节点（我们这里是core节点）上找到kafka-broker的安装目录，执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh bin/kafka-simple-consumer-shell.sh --topic online_userbehaviortrack --offset -2 --broker-list 10.68.160.52:6667,10.68.160.53:6667,10.68.160.54:6667 --print-offsets --partition 9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的命令会输出类似&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;next offset = 3381
{&amp;quot;message&amp;quot;:&amp;quot;2017-01-11T14:40:55+0800`1484116855552`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;的结果，从中可以很详细的看到每条message的offset，结合管道和less就可以根据时间找到出现脏数据的offset，然后确定一个时间点，比如下午2点，在每个partition中分别找到2点之前的最后一个offset，找个地方记下来。&lt;/p&gt;

&lt;h3 id=&#34;2-到zookeeper中找到每个对应的partiton-用步骤1的offset结果覆盖partiton中的offset字段&#34;&gt;2. 到ZooKeeper中找到每个对应的Partiton，用步骤1的offset结果覆盖Partiton中的offset字段&lt;/h3&gt;

&lt;p&gt;因为我的KafkaSpout是这么写的&lt;code&gt;SpoutConfig kafkaSpoutConfig = new SpoutConfig(brokerHosts, topic, &amp;quot;/&amp;quot; + topic, client_id);&lt;/code&gt;，所以在我的ZooKeeper中我应该去&lt;code&gt;/mytopic/mytopologyname/&lt;/code&gt;中找到所有的partiton，这个需要根据你自己的代码来确定。&lt;/p&gt;

&lt;p&gt;以partition_9为例，结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[zk: localhost:2181(CONNECTED) 2] get /mytopic/fe_analyze/partition_9
{&amp;quot;topology&amp;quot;:{&amp;quot;id&amp;quot;:&amp;quot;c75ac929-1a8e-4958-a637-ead9a95436ca&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;mytopologyname&amp;quot;},&amp;quot;offset&amp;quot;:3384,&amp;quot;partition&amp;quot;:9,&amp;quot;broker&amp;quot;:{&amp;quot;host&amp;quot;:&amp;quot;kmr-9a387314-gn-6bb9c438-core-1-002.ksc.com&amp;quot;,&amp;quot;port&amp;quot;:6667},&amp;quot;topic&amp;quot;:&amp;quot;mytopic&amp;quot;}
cZxid = 0x1017bd837
ctime = Mon Dec 26 15:06:19 CST 2016
mZxid = 0x101cbb4ec
mtime = Wed Jan 11 14:49:40 CST 2017
pZxid = 0x1017bd837
cversion = 0
dataVersion = 2173
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 223
numChildren = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为我们只需要设置partiton的offset值，而它接收一个JSON类型的值，所以就需要这样&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[zk: localhost:2181(CONNECTED) 4] set /mytopic/mytopologyname/partition_9 {&amp;quot;topology&amp;quot;:{&amp;quot;id&amp;quot;:&amp;quot;c75ac929-1a8e-4958-a637-ead9a95436ca&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;fe_analyze&amp;quot;},&amp;quot;offset&amp;quot;:3386,&amp;quot;partition&amp;quot;:9,&amp;quot;broker&amp;quot;:{&amp;quot;host&amp;quot;:&amp;quot;kmr-9a387314-gn-6bb9c438-core-1-002.ksc.com&amp;quot;,&amp;quot;port&amp;quot;:6667},&amp;quot;topic&amp;quot;:&amp;quot;mytopic&amp;quot;}
cZxid = 0x1017bd837
ctime = Mon Dec 26 15:06:19 CST 2016
mZxid = 0x101cbba18
mtime = Wed Jan 11 14:54:29 CST 2017
pZxid = 0x1017bd837
cversion = 0
dataVersion = 2176
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 223
numChildren = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把其中的offset的值替换成在步骤1中拿到的partiton 9的offset值。&lt;/p&gt;

&lt;h3 id=&#34;3-写脚本删除hdfs中对应topology出现脏数据以后的所有数据&#34;&gt;3. 写脚本删除HDFS中对应Topology出现脏数据以后的所有数据&lt;/h3&gt;

&lt;p&gt;我的设计是每天分成24个partition，类似(yyyymmddhh=2017011010)这种，所以就可以写个类似这样的脚本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/usr/bin/env bash

DAY=$(date -d &amp;quot;-1 day&amp;quot; +%Y%m%d)

for HH in {00..23}
do
    hdfs dfs -rm -r -f /path/to/topology/yyyymmddhh=${DAY}${HH}
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个脚本可以删除前一天的所有24个partiton的数据。&lt;/p&gt;

&lt;h3 id=&#34;4-如果新的topology对应的hive-table也有变化-需要先修改hive的表结构&#34;&gt;4. 如果新的Topology对应的Hive table也有变化，需要先修改Hive的表结构&lt;/h3&gt;

&lt;p&gt;Hive表结构的修改和MySQL的差不多，不过还是有些差别：比如新添加的列只能放在所有真实列的最后，partiton伪列的前面。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;alter table xxxx add columns (string coln, string colm);&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;注意是&lt;strong&gt;&lt;code&gt;columns&lt;/code&gt;&lt;/strong&gt;而不是&lt;code&gt;column&lt;/code&gt;。而且也不能指定after哪个column。&lt;/p&gt;

&lt;h3 id=&#34;5-重新添加所有受影响的partiton到hive-table&#34;&gt;5. 重新添加所有受影响的partiton到Hive table&lt;/h3&gt;

&lt;p&gt;因为我们这里用的是external table（当我提到HDFS的时候你就应该知道了），所以所有新数据对应的partiton都应该删除后重新添加，以yyyymmddhh=2017011020为例，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;删除partition &lt;code&gt;alter table xxxx drop if exists partiton (yyyymmddhh= 2017011020);&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加partition &lt;code&gt;alter table xxxx add if not exists partition (yyyymmddhh= 2017011020);&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样就可以对新加进来的数据进行搜索了。你可以试一下，如果少了这步操作，在新的Hive table里新加的列的值会全部是NULL。&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;步骤很多，但操作还是挺简单的，而且需要细心，不能出错，尤其这里每一步都是线上的操作，一次微小的出错都可能酿成事故。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;找到每个partiton出现脏数据时的offset&lt;/li&gt;
&lt;li&gt;把当前每个partiton的offset设置成出现脏数据时的offset&lt;/li&gt;
&lt;li&gt;清除出错的脏数据&lt;/li&gt;
&lt;li&gt;修改Hive table（如果需要）&lt;/li&gt;
&lt;li&gt;删除并重新添加受影响的partition(如果需要)&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>在本地单机部署Hadoop/Storm运行环境</title>
      <link>http://lovelock.coding.me/java/deploy-pseudo-distributed-mode-hadoop/</link>
      <pubDate>Mon, 10 Oct 2016 17:53:14 +0800</pubDate>
      
      <guid>http://lovelock.coding.me/java/deploy-pseudo-distributed-mode-hadoop/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;由于要在小组内做一个关于Storm的分享，涉及到我负责开发的大数据项目，本着开放的原则，把我做的准备工作记录下来，提前发给可能参会的同事。&lt;br /&gt;
要演示的项目目前为止用到了Apache的多个项目，包括Kafka, Storm, Hadoop(HDFS, Hive), ZooKeeper等，项目刚刚起步，很多基础设施还不完善，比如现在是在本地开发完成之后直接部署到线上环境的，这次演示可不能直接在线上环境做了，故而在本地的台式机上部署了一下&lt;strong&gt;伪集群&lt;/strong&gt;，用来作为演示和以后开发测试用的环境。&lt;/p&gt;

&lt;h2 id=&#34;准备工作&#34;&gt;准备工作&lt;/h2&gt;

&lt;p&gt;说明：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;以下工作全部基于Ubuntu 16.04。用其他的发行版或版本理论上应该都是可行的，可能有些命令需要微调。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;所有需要执行的命令前面都有一个&lt;code&gt;$&lt;/code&gt;，表示的是Bash的命令提示符。&lt;/li&gt;
&lt;li&gt;下载安装包时我使用了速度相对较快的国内镜像，如果你对此有任何异议，可以自行去中心镜像站点下载。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;默认情况下文中提到的所有如StormUI等控制后台访问的路径都是localhost，如果你需要从Linux主机外部访问，需要iptables放行相应的端口。在本文中，用的Web端口有三个，具体如下表：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;服务描述&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;端口号&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Storm UI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8080&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Hadoop UI&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50070&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Hadoop Applications&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8088&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;以Ubuntu为例，需要执行以下命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo ufw allow 8080/tcp
$ sudo ufw allow 50070/tcp
$ sudo ufw allow 8088/tcp
$ sudo ufw reload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CentOS/RHEL 7.0以下可能需要直接操作iptables，7.0以上可以使用&lt;code&gt;firewall-cmd&lt;/code&gt;进行类似的操作。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;1-创建独立的用户并赋予合适的权限&#34;&gt;1. 创建独立的用户并赋予合适的权限&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo useradd hadoop
$ sudo passwd hadoop
$ sudo chsh hadoop
$ sudo mkdir /home/hadoop
$ sudo chown -R hadoop:hadoop /opt/apache
$ sudo chown -R hadoop:hadoop /home/hadoop
$ su - hadoop
$ ssh-keygen
$ cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys
$ chmod 0600 ~/.ssh/authorized_keys
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;FAQ：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;为什么需要chsh并且手动为用户创建家目录？&lt;br /&gt;
不知道为什么在 Ubuntu 环境中执行&lt;code&gt;useradd&lt;/code&gt;命令之后并不会给新建的用户创建家目录和设置shell，可能是为了安全吧，要稍微麻烦一些。&lt;/li&gt;
&lt;li&gt;为什么不给新用户赋予root权限？&lt;br /&gt;
这里是测试环境倒还好，如果你需要搭建生产环境，记住千万不要给hadoop用户赋予root权限，它需要哪些目录的权限就单独赋予即可，它需要运行的端口都是1024以上的，都不需要root权限。如果需要，执行&lt;code&gt;sudo gpasswd -a hadoop sudo&lt;/code&gt;即可。&lt;/li&gt;
&lt;li&gt;为什么要做一个密钥认证？&lt;br /&gt;
这是因为在后续的步骤中会有&lt;strong&gt;从本地用户登录本地用户&lt;/strong&gt;的需求，也就是执行了&lt;code&gt;ssh hadoop@localhost&lt;/code&gt;这个命令，如果不做密钥信任，会需要多次输入密码。&lt;/li&gt;
&lt;li&gt;为什么会有一个&lt;code&gt;/opt/apache&lt;/code&gt;目录？&lt;br /&gt;
往下看。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;1-下载所需安装包&#34;&gt;1. 下载所需安装包&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;下载Java&lt;br /&gt;
到&lt;a href=&#34;http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html&#34;&gt;Java官网&lt;/a&gt;下载适用于Linux的Java安装包（这里下载了jdk-8u101-linux-x64.tar.gz）。&lt;/li&gt;
&lt;li&gt;下载maven&lt;br /&gt;
点击&lt;a href=&#34;http://mirrors.aliyun.com/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.zip&#34;&gt;阿里云镜像站&lt;/a&gt;下载最新版Maven。&lt;/li&gt;
&lt;li&gt;下载Apache的Storm Hadoop Kafka ZooKeeper&lt;br /&gt;
&lt;a href=&#34;http://mirrors.aliyun.com/apache/storm/apache-storm-0.10.2/apache-storm-0.10.2.tar.gz&#34;&gt;点击下载Storm-0.10.2&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://mirrors.aliyun.com/apache/hadoop/core/hadoop-2.6.4/hadoop-2.6.4.tar.gz&#34;&gt;点击下载Hadoop-2.6.4&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://mirrors.aliyun.com/apache/kafka/0.8.2.2/kafka_2.10-0.8.2.2.tgz&#34;&gt;点击下载Kafka_2.10-0.8.2.2&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://mirrors.aliyun.com/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz&#34;&gt;点击下载ZooKeeper-3.4.6&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;2-创建所需目录&#34;&gt;2. 创建所需目录&lt;/h3&gt;

&lt;p&gt;把所有和此项目相关的文件都放在统一的位置。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ sudo mkdir -p /opt/apache/{jdk, storm,hadoop,kafka,zookeeper}&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;3-解压所有安装包-将相应的安装包放在对应的位置&#34;&gt;3. 解压所有安装包，将相应的安装包放在对应的位置&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mv jdk1.8.0_101/* /opt/jdk/.
$ sudo mv apache-storm-0.10.0/* /opt/apache/storm/.
$ sudo mv hadoop-2.6.4/* /opt/apache/hadoop/.
$ sudo mv kafka_2.10-0.8.2.2/* /opt/apache/kafka/.
$ sudo mv zookeeper-3.4.6/* /opt/apache/zookeeper
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-配置环境变量&#34;&gt;4. 配置环境变量&lt;/h3&gt;

&lt;p&gt;在~/.bashrc中添加下面的片段&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export JAVA_HOME=/opt/jdk
export APACHE_HOME=/opt/apache
export STORM_HOME=$APACHE_HOME/storm
export ZK_HOME=$APACHE_HOME/zookeeper
export KAFKA_HOME=$APACHE_HOME/kafka

export HADOOP_HOME=$APACHE_HOME/hadoop
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_INSTALL=$HADOOP_HOME

export PATH=$PATH:$JAVA_HOME/bin
export PATH=$PATH:$STORM_HOME/bin
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export PATH=$PATH:$ZK_HOME/bin
export PATH=$PATH:$KAFKA_HOME/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后执行&lt;code&gt;$ source ~/.bashrc&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;5-安装java&#34;&gt;5. 安装Java&lt;/h4&gt;

&lt;p&gt;Apache的这些东西都是直接放在对的地方就可以运行的，但Java需要稍微的配置一下，因为可能你的机器上已经装了OpenJDK。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ sudo update-alternatives --install /usr/local/bin/java java /opt/jdk/bin/java 10000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这时再执行&lt;code&gt;$ java -version&lt;/code&gt;验证一下是否使用的时最新安装的JDK。&lt;/p&gt;

&lt;h2 id=&#34;2-启动服务&#34;&gt;2. 启动服务&lt;/h2&gt;

&lt;h3 id=&#34;1-启动zookeeper&#34;&gt;1. 启动ZooKeeper&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cp $ZK_HOME/conf/zoo_sample.cfg $ZK_HOME/conf/zoo.cfg
$ zkServer.sh start
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-启动storm&#34;&gt;2. 启动Storm&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ storm nimbus
$ storm supervisor
$ storm ui
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在可以访问&lt;a href=&#34;http://localhost:8080&#34;&gt;Storm UI&lt;/a&gt;了。&lt;/p&gt;

&lt;h3 id=&#34;3-启动kafka-broker&#34;&gt;3. 启动Kafka Broker&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cd $KAFKA_HOME
$ bin/kafka-server-start.sh -daemon config/server.properties
$ bin/kafka-topics.sh --list --zookeeper localhost:2181
$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 10 --topic storm-demo-topic
Created topic &amp;quot;storm-demo-topic&amp;quot;.
$ bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic storm-demo-topic
Topic:storm-demo-topic  PartitionCount:10       ReplicationFactor:1     Configs:
        Topic: storm-demo-topic Partition: 0    Leader: 0       Replicas: 0     Isr: 0
        Topic: storm-demo-topic Partition: 1    Leader: 0       Replicas: 0     Isr: 0
        Topic: storm-demo-topic Partition: 2    Leader: 0       Replicas: 0     Isr: 0
        Topic: storm-demo-topic Partition: 3    Leader: 0       Replicas: 0     Isr: 0
        Topic: storm-demo-topic Partition: 4    Leader: 0       Replicas: 0     Isr: 0
        Topic: storm-demo-topic Partition: 5    Leader: 0       Replicas: 0     Isr: 0
        Topic: storm-demo-topic Partition: 6    Leader: 0       Replicas: 0     Isr: 0
        Topic: storm-demo-topic Partition: 7    Leader: 0       Replicas: 0     Isr: 0
        Topic: storm-demo-topic Partition: 8    Leader: 0       Replicas: 0     Isr: 0
        Topic: storm-demo-topic Partition: 9    Leader: 0       Replicas: 0     Isr: 0
$ kafka-console-producer.sh --broker-list localhost:9092 --topic storm-demo-topic
$ kafka-console-consumer.sh --zookeeper localhost:2181 --topic storm-demo-topic --from-beginning
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;注意前面有些命令仅仅是为了测试kafka broker运行的情况。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;4-启动hadoop-hdfs&#34;&gt;4. 启动Hadoop(HDFS)&lt;/h3&gt;

&lt;h4 id=&#34;1-修改hadoop的配置文件&#34;&gt;1. 修改Hadoop的配置文件&lt;/h4&gt;

&lt;p&gt;在&lt;code&gt;$HADOOP_HOME/etc/hadoop&lt;/code&gt;目录下有5个文件需要修改:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;core-site.xml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;fs.default.name&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hdfs://localhost:9000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;hdfs-site.xml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;

    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.name.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;file:///home/hadoop/hadoopinfra/hdfs/namenode&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.data.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;file:///home/hadoop/hadoopinfra/hdfs/datanode&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;

&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;yarn-site.xml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;mapred-site.xml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;hadoop-env.sh&lt;/code&gt;
把hadoop-env.sh中的${JAVA_HOME}替换成路径,这里是&lt;code&gt;/opt/jdk&lt;/code&gt;，因为貌似会找不到正确的&lt;code&gt;JAVA_HOME&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;2-验证是否安装成功&#34;&gt;2. 验证是否安装成功&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ hadoop namenode -format
$ start-dfs.sh
$ start-yarn.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行上面三行语句，观察有没有明显的报错信息。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://localhost:50070&#34;&gt;检查HadoopUI&lt;/a&gt;运行是否正常。&lt;br /&gt;
&lt;a href=&#34;http://localhost:8088&#34;&gt;检查Hadoop Applications&lt;/a&gt;(我自己取的名字)运行是否正常。&lt;/p&gt;

&lt;p&gt;至此已经搭建了一个可以运行的hadoop环境，可以移步这里查看关于Storm入门分享详情。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
