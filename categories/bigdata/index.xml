<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Me &amp; Web</title>
    <link>http://unixera.com/categories/bigdata/index.xml</link>
    <description>Recent content on Me &amp; Web</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>frostwong@gmail.com (Frost Wong)</managingEditor>
    <webMaster>frostwong@gmail.com (Frost Wong)</webMaster>
    <copyright>(c) 2013-2016 Frost Wong. All rights reserved.</copyright>
    <atom:link href="/categories/bigdata/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ZooKeeper原理简介和简单使用</title>
      <link>http://unixera.com/bigdata/zookeeper-simple-practice/</link>
      <pubDate>Wed, 12 Oct 2016 23:36:09 +0800</pubDate>
      <author>frostwong@gmail.com (Frost Wong)</author>
      <guid>http://unixera.com/bigdata/zookeeper-simple-practice/</guid>
      <description>

&lt;h2 id=&#34;什么是分布式应用&#34;&gt;什么是分布式应用&lt;/h2&gt;

&lt;p&gt;分布式应用运行在其上的一组系统被称为『集群』(cluster)，运行在集群中的每个机器被称为『节点』(node)。&lt;/p&gt;

&lt;h3 id=&#34;分布式应用的优点&#34;&gt;分布式应用的优点&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;可靠性 单机或少量机器的故障不会导致整个系统不可用。&lt;/li&gt;
&lt;li&gt;可扩展性 不用停机只需要做很少的配置就可以根据需求通过增加机器来提升系统的性能。&lt;/li&gt;
&lt;li&gt;透明性 隐藏了系统的复杂性，对外值暴露单一的入口/应用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;分布式应用需要解决的难点&#34;&gt;分布式应用需要解决的难点&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;竞争条件 两个或多个机器都尝试去执行同一个任务，而该任务在任意时刻都应该只被一台机器执行。比如，共享的资源在某一时刻应该只能被一台机器修改。&lt;/li&gt;
&lt;li&gt;死锁 两个或多个操作无限期的相互等待对方完成。&lt;/li&gt;
&lt;li&gt;不一致性 数据的部分错误。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;zookeeper简介&#34;&gt;ZooKeeper简介&lt;/h2&gt;

&lt;p&gt;ZooKeeper是一个&lt;strong&gt;分布式的&lt;/strong&gt;、用来管理大量主机的&lt;strong&gt;协调服务&lt;/strong&gt;。
在分布式环境中协调和管理一个服务是很复杂的工作，而ZooKeeper用简单的架构和API解决了这个问题，它用&lt;code&gt;fail-safe synchronization&lt;/code&gt;机制解决了竞争和死锁的问题, 用&lt;code&gt;atomicity(原子性)&lt;/code&gt;解决了数据的一致性问题。它屏蔽了分布式环境中的复杂性，让开发人员可以专注于核心应用功能的开发，而不用去关心分布式环境的太多细节。&lt;/p&gt;

&lt;h3 id=&#34;zookeeper提供的服务&#34;&gt;ZooKeeper提供的服务&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;名字服务 在一个集群内根据name找到主机，类似DNS服务&lt;/li&gt;
&lt;li&gt;配置管理 集中管理某个节点的最新配置&lt;/li&gt;
&lt;li&gt;集群管理 管理一个集群中某一节点的加入和离开&lt;/li&gt;
&lt;li&gt;主节点选举 协调一个集群选举中一个新的主节点&lt;/li&gt;
&lt;li&gt;加锁和同步服务 在数据被修改时给其加锁，这种机制可以帮助你在连接到其他如HBase的分布式服务时实现自动错误恢复&lt;/li&gt;
&lt;li&gt;存放高可用数据 可以保证在一个或多个节点出故障时保证数据的可用性&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;zookeeper的优点&#34;&gt;ZooKeeper的优点&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;简单的分布式协调过程&lt;/li&gt;
&lt;li&gt;同步 服务器进程间的互斥和协作&lt;/li&gt;
&lt;li&gt;有序的消息&lt;/li&gt;
&lt;li&gt;序列化 用指定的规则编码数据。保证你的应用一致的运行。这种方式可以用在MapReduce中来协调对来执行线程&lt;/li&gt;
&lt;li&gt;可靠性&lt;/li&gt;
&lt;li&gt;原子性 数据传输要么成功要么失败，不存在中间状态&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;zookeeper的架构&#34;&gt;ZooKeeper的架构&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://ww3.sinaimg.cn/large/006y8mN6jw1f7alv0grqej30fw04zdgg.jpg&#34; alt=&#34;ZooKeeper架构图&#34; /&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;概念&lt;/th&gt;
&lt;th&gt;职责和作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Client&lt;/td&gt;
&lt;td&gt;Client定时向Server发送消息通知Server该Client是alive众泰，同时Server会返回Response给Client，如果Client发送Message后没有收到Response，则会自动重定向到其他Server&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Server&lt;/td&gt;
&lt;td&gt;ZooKeeper集群中的一个节点，提供给Clients所有的服务&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Ensemble&lt;/td&gt;
&lt;td&gt;一个可以提供ZooKeeper服务的集群，如果要达到高可用性，至少需要三个节点&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Leader&lt;/td&gt;
&lt;td&gt;节点故障时执行自动恢复的节点，启动时选举出的&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Follower&lt;/td&gt;
&lt;td&gt;根据Leader的指示执行任务&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;层级的命名空间&#34;&gt;层级的命名空间&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://ww4.sinaimg.cn/large/006y8mN6jw1f7ayltmo57j30go0ce0t8.jpg&#34; alt=&#34;ZooKeeper的层级的命名空间&#34; /&gt;&lt;/p&gt;

&lt;p&gt;层级结构中的每个节点叫做znode, 每个znode维护一个&lt;code&gt;stat&lt;/code&gt;结构。这个&lt;code&gt;stat&lt;/code&gt;仅仅提供一个znode的元信息，其中包括版本号(Version number)、行为控制列表(Action Control List, ACL)、时间戳(Timestamp)、数据长度(Data Lenght)。&lt;/p&gt;

&lt;p&gt;下面来就实例看一下一个znode有哪些信息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ww4.sinaimg.cn/large/006y8mN6jw1f8r0ho7jkhj31320h6ad1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这样一看就很明显了吧。&lt;/p&gt;

&lt;h3 id=&#34;znode的类型&#34;&gt;znode的类型&lt;/h3&gt;

&lt;p&gt;znode分为三种类型：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;永久型 永久型节点当客户端断开连接之后仍然存在，默认情况下创建的节点都是永久型节点&lt;/li&gt;
&lt;li&gt;临时型 只有client保持alive时才存在的节点叫临时节点，当client从ZooKeeper集群断开时，节点被自动删除。所以临时节点不允许有子节点。如果一个临时节点被删除了，下一个合适的节点会填充它的位置。临时节点在Leader的选取中起到重要作用。&lt;/li&gt;
&lt;li&gt;顺序型 序列型节点可以是永久的也可以是临时的。当一个znode被创建为顺序型时，ZooKeeper在它原来的name后面加上十位的十进制数字。如果两个顺序型节点是并发创建的，ZooKeeper会保证两个节点的name不同。顺序型节点在锁和同步中起到重要作用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;会话-sessions&#34;&gt;会话（Sessions）&lt;/h3&gt;

&lt;p&gt;一个会话中的请求是按照FIFO的顺序执行的。当一个client连接上一个server，一个会话就创建成功了并且会生成一个session id给client。&lt;br /&gt;
client会按一个时间间隔给server发送heartbeat来保证session的有效性。如果在一个session的生命周期内没有收到client的heartbeat，它就会认为这个client已经死掉了。&lt;br /&gt;
Session超时通常用ms表示。当一个session不管由于什么原因结束时，在session中创建的临时节点都会被删除掉。&lt;/p&gt;

&lt;h3 id=&#34;watches&#34;&gt;Watches&lt;/h3&gt;

&lt;p&gt;Watches是用来保证client能在znode上的数据发生变化时收到通知的一种简单机制。client在读取znode的数据时可以设置一个watches给一个特定的znode，当这个znode上的数据或者它的子节点发生变化时都会触发watches给client发送通知。&lt;br /&gt;
watches只会被触发一次，如果client还需要通知，那就需要另外一次的读取操作了。当一个client和server之间的会话过期时，它们之间的连接就断开了，同时watches也会被移除。&lt;/p&gt;

&lt;h3 id=&#34;工作流程&#34;&gt;工作流程&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;client读取数据 client发送一个&lt;strong&gt;读取请求&lt;/strong&gt;给ZooKeeper的一个节点，该节点根据请求中的path信息读取&lt;strong&gt;自己数据库中的数据&lt;/strong&gt;返回znode的信息给client。所以读取操作在ZooKeeper集群中是很快的。&lt;/li&gt;
&lt;li&gt;client写数据 如果收到请求的是Follower，它会先把请求转发给Leader，由Leader再发送写请求给Followers。只有&lt;strong&gt;大多数节点&lt;/strong&gt;正确响应时，写请求才会成功并且返回正确的返回码给client。否则写请求就会失败。这个严格的&lt;strong&gt;大多数节点&lt;/strong&gt;被称为&lt;strong&gt;Quorum（法定人数)&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;zookeeper中的节点数量&#34;&gt;ZooKeeper中的节点数量&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;当只有一个节点时，没有大多数&lt;/li&gt;
&lt;li&gt;只有两个节点，一个出故障时，也没有大多数&lt;/li&gt;
&lt;li&gt;当有三个节点，有一个出了故障，那2个就是大多数&lt;/li&gt;
&lt;li&gt;当有四个节点，2个出故障了，那也是没有大多数的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以，ZooKeeper集群的中节点的数量不要太多，不然写的性能会有下降。同时节点的数量是3/5/7这种奇数，而不要是偶数。&lt;/p&gt;

&lt;h3 id=&#34;小结&#34;&gt;小结&lt;/h3&gt;

&lt;p&gt;看了上面的这么一大套理论，可能还是对ZooKeeper做的事情云里雾里，因为它做的事情太抽象了，好像实际它什么都没做，但又发现好像每个组件比如Kafka/Storm都要和ZooKeeper配合才能用。到底为什么呢？&lt;/p&gt;

&lt;p&gt;上面讲过，ZooKeeper其实是一个配置分发服务，也就是具体的应用如Kafka和Storm都是&lt;strong&gt;无状态&lt;/strong&gt;的，它本身为了保持&lt;strong&gt;容错&lt;/strong&gt;的特性，而容错很重要的一项特性就是应用Down掉之后重启还要能从之前结束时的地方继续。既然是无状态，其实是&lt;strong&gt;自己不存储状态&lt;/strong&gt;，那要实现的这个特性肯定是&lt;strong&gt;需要知道&lt;/strong&gt;应用Down掉之前的状态的，那么好，我就把状态存在ZooKeeper里。&lt;/p&gt;

&lt;p&gt;举个生动的例子，假设有一个很长的（水）槽，Kafka会每秒把一个玻璃球放在槽里，这样的结果就是最先放进去的玻璃球在最前面。而Storm就是&lt;strong&gt;计数工&lt;/strong&gt;，（注意&lt;strong&gt;不是搬运工&lt;/strong&gt;，因为它数了之后并不会真实的改变玻璃球的位置）极端一点这个游戏是在一个战场上，Storm随时会死掉，它怎么保证它的后来者来到之后马上知道它之前数到哪个位置了？自己当然是不可靠的，因为它死掉之后这个信息就丢失了，所以它&lt;strong&gt;每数一个&lt;/strong&gt;就朝ZooKeeper大喊一声（发送写请求）告诉它数到哪个位置了，而Storm又是个健忘症（无状态），刚数完的自己就忘了，更不用说后面来的人了，那么当它把位置信息告诉ZooKeeper之后其实它和自己的后来者就没有区别了，因为不管是谁，在计数之前都需要先去ZooKeeper读取一下前面数到的位置。这样的好处就是每个Storm随时都可以死掉，只要能有新的应用随时可以起来即可。那么存到了ZooKeeper就万无一失了么？考虑前面ZooKeeper处理写请求的特点，它是把相同的信息在集群中所有的机器上都写了一份，即使其中的一台或几台宕掉了，除非在这几台重启之前仅剩的一台也宕掉了，服务是不受影响的。如果全宕掉了，那真的没办法了，你把整个机房的电源拔掉，肯定会丢数据的。&lt;/p&gt;

&lt;p&gt;也可以理解为把状态和应用做的解耦。&lt;/p&gt;

&lt;p&gt;那么问题来了，为什么是在战场上？为什么好好的一个应用会无缘无故的Down掉呢？这就要从分布式应用的特点说起了。我们知道以前的所谓大型机、小型机都是很大很昂贵的特殊机器，是区别于普通的硬件的，包括CPU、内存、硬盘都是特制的，所以一台机器上百万甚至千万的都很常见，这种机器宕机的几率很小，但如果宕机的话影响也会相当严重。所以可以说这些机器的费用里其实也包含了保险费，因为机器宕机导致的损失，供应商是要负责任的。&lt;/p&gt;

&lt;p&gt;但现在不同了，现在是用大量普通（廉价）的机器组成集群来替代之前特殊的机器，既然是普通，那出错的几率当然就更高了，这也就是为什么诸如Storm这些系统在设计之初就特别注重&lt;strong&gt;容错&lt;/strong&gt;和&lt;strong&gt;无状态&lt;/strong&gt;了。&lt;/p&gt;

&lt;h2 id=&#34;zookeeper的使用&#34;&gt;ZooKeeper的使用&lt;/h2&gt;

&lt;h3 id=&#34;安装和配置&#34;&gt;安装和配置&lt;/h3&gt;

&lt;h4 id=&#34;安装&#34;&gt;安装&lt;/h4&gt;

&lt;p&gt;大数据的这套东西安装起来都是很简单，因为都是编译好的包，直接解压之后就可以以默认配置执行了。不过ZooKeeper有点特殊，因为它需要读取的配置文件是&lt;code&gt;conf/zoo.cfg&lt;/code&gt;，而默认的发行包里面是有个&lt;code&gt;conf/zoo_sample.cfg&lt;/code&gt;，不过好在只需要重命名一下即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost zookeeper-3.4.8]# cp conf/zoo_sample.cfg conf/zoo.cfg
[root@localhost zookeeper-3.4.8]# bin/zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /root/packages/zookeeper-3.4.8/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@localhost zookeeper-3.4.8]# bin/zkServer.sh status
Mode: standalone
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;注意这里的Mode，表示单点模式，区别于集群模式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;配置&#34;&gt;配置&lt;/h4&gt;

&lt;p&gt;前面只讲了基础配置，这样的配置是没法跑集群环境的，下面先从默认配置出发，一步一步搭建一个集群环境。
先贴默认配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tickTime=2000
initLimit=10
syncLimit=5
dataDir=/tmp/zookeeper
clientPort=2181
#maxClientCnxns=60
#autopurge.snapRetainCount=3
#autopurge.purgeInterval=1
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;tickTime&lt;/code&gt;： ZooKeeper服务器或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime就会发送一条心跳&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataDir&lt;/code&gt; 顾名思义就是ZooKeeper保存数据的目录&lt;/li&gt;
&lt;li&gt;&lt;code&gt;clientPort&lt;/code&gt; ZooKeeper对外提供服务的端口，即客户端通过该端口与ZooKeeper通信&lt;/li&gt;
&lt;li&gt;&lt;code&gt;initLimit&lt;/code&gt; ZooKeeper集群中的Leader忍受Follower多少个心跳间隔不发送心跳。从这里的默认配置推算，10个心跳间隔，每个心跳间隔2秒钟，也就是当Leader经过2*10秒还收不到Follower的信条时就认为这个Follower已经挂了&lt;/li&gt;
&lt;li&gt;&lt;code&gt;syncLimit&lt;/code&gt; Leader和Follower之间发送消息时，请求和应答的时间长度，默认5，及10秒&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;从上面的描述就可以看到，从第4条开始就是集群需要的配置了，然而仅仅在每个机器上这样配置并不能变成一个集群，还需要一个重要的配置，形如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server.1=c1:2888:3888
server.2=c2:2888:3888
server.3=c3:2888:3888
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中的&lt;code&gt;server.n&lt;/code&gt;中的n表示节点的编号，那么问题来了，编号从哪里定义呢？我觉得这个设计其实不太好，当然我也想不到更好的方式来解决这个问题了。我们还需要在&lt;code&gt;dataDir&lt;/code&gt;中写入一个名为&lt;code&gt;myid&lt;/code&gt;的文件，其中填写当前机器的编号，操作如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /path/to/dataDir
echo 1 &amp;gt; myid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c1的位置是节点机器的hostname或者IP地址，这样写当然还是不行的，因为它们并不知道c1是什么鬼，所以还需要修改&lt;code&gt;/etc/hosts&lt;/code&gt;，以我当前的本地集群为例，在该文件中添加&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.1.111 c1
192.168.1.110 c2
192.168.1.112 c3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2888是默认的Follower与Leader交换信息的端口，3888是用于选举Leader的端口，当Leader挂了，当然需要选举一个新的Leader来继续它未竟的事业了。&lt;/p&gt;

&lt;h3 id=&#34;启动集群&#34;&gt;启动集群&lt;/h3&gt;

&lt;p&gt;这时可以在三台机器上同时执行&lt;code&gt;bin/zkServer.sh start&lt;/code&gt;了。如果看到和前面一样的结果（注意把刚才已经启动的服务先关掉，执行&lt;code&gt;bin/zkServer.sh stop&lt;/code&gt;），恭喜你成功了一半了。
这时再执行&lt;code&gt;bin/zkServer.sh status&lt;/code&gt;你会惊奇的发现其中一台会显示&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost zookeeper-3.4.8]# bin/zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /root/packages/zookeeper-3.4.8/bin/../conf/zoo.cfg
Mode: leader
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外两台显示&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@localhost zookeeper-3.4.8]# bin/zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /root/packages/zookeeper-3.4.8/bin/../conf/zoo.cfg
Mode: follower
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了证明Leader节点是自动选举的，可以把Leader手动关掉，再分别看看另外两台的&lt;code&gt;status&lt;/code&gt;。是不是有一个变成了Leader了？&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;几天前写这篇文章时没有问题，但今天继续编写Kafka部分时，因为之前重启过这几台虚拟机，IP变了，重启所有虚拟机之后发现虽然已经在本地启动了ZooKeeper服务，但执行&lt;code&gt;zkServer.sh status&lt;/code&gt;时总是提示没有运行，而如果你再执行&lt;code&gt;start&lt;/code&gt;指令，它又会提示说进程已经在运行了。根据&lt;a href=&#34;http://stackoverflow.com/questions/29909191/zookeeper-it-is-probably-not-running&#34;&gt;StackOverFlow的答案&lt;/a&gt;，把&lt;code&gt;/root/packages/zookeeper-3.4.8/bin&lt;/code&gt;添加至&lt;code&gt;$PATH&lt;/code&gt;中即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim /etc/profile.d/zookeeper.sh
export ZK_HOME=/root/packages/zookeeper-3.4.8
export PATH=$PATH:$ZK_HOME/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后执行&lt;code&gt;source /etc/profile.d/zookeeper.sh&lt;/code&gt;即可直接在系统的任何地方执行&lt;code&gt;zkServer.sh start&lt;/code&gt;了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过动态的选举Leader节点，就解决了&lt;strong&gt;主从系统的单点故障问题&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&#34;简单使用&#34;&gt;简单使用&lt;/h3&gt;

&lt;p&gt;前面说了启动服务，细心的你可能还发现在bin目录里面还有一个zkCli.sh（请自动无视zkCli.cmd，因为那明显是给Windows用的，而我觉得也没有人会在Windows上跑这些服务），这就是ZooKeeper的命令行客户端。&lt;/p&gt;

&lt;p&gt;而我要说的只有两个最简单的命令。&lt;/p&gt;

&lt;h4 id=&#34;1-ls&#34;&gt;1. &lt;code&gt;ls&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;ls&lt;/code&gt;顾名思义就是查看指定path下的数据，前面我已经演示过了，要注意的一点是&lt;strong&gt;如果&lt;code&gt;ls&lt;/code&gt;后跟的是一个叶子节点，返回的结果是&lt;code&gt;[]&lt;/code&gt;&lt;/strong&gt;，这时你应该很敏锐的意识到应该换用&lt;code&gt;get&lt;/code&gt;来操作这个节点从而查看它的详细信息了。&lt;/p&gt;

&lt;h4 id=&#34;2-get&#34;&gt;2. &lt;code&gt;get&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;get&lt;/code&gt;当然就是用来查看指定节点的详细信息用的了。&lt;/p&gt;

&lt;p&gt;ZooKeeper提供的借口当然远远不止这两个，但起码到目前为止我还没有用到需要自行调用ZooKeeper接口的地方。因为实际上ZooKeeper是一个很底层的服务，它是用来为Storm和Kafka这类系统提供服务的，而我们通常不直接使用它们。在前两天一次查问题的过程中，发现数据一直在重复写入HDFS，查到了一个症状是ZooKeeper中的offset从一次重启发布之后一直没有更新过，导致系统一直反复读取该时间点之后的数据。这期间也就只用了这两个命令，至于对各种语言的binding，这里就不多说了，如果你要使用ZooKeeper给你的应用提供服务，那也不是看我的这篇文章就能搞明白的：）&lt;/p&gt;

&lt;p&gt;Happy Coding!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;本文大量参考了&lt;a href=&#34;https://www.tutorialspoint.com//zookeeper/index.htm&#34;&gt;https://www.tutorialspoint.com/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>一个真实Storm应用源码解析</title>
      <link>http://unixera.com/java/storm-demo-presentation/</link>
      <pubDate>Tue, 11 Oct 2016 16:41:18 +0800</pubDate>
      <author>frostwong@gmail.com (Frost Wong)</author>
      <guid>http://unixera.com/java/storm-demo-presentation/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这里是Storm分享的内容。我自己也是初学者，这里抛砖引玉，希望大家多多指教。为简单起见，本应用用的是Java实现，没有用到Storm的多语言支持和更高层面的Trident Topology。源码详见&lt;a href=&#34;https://github.com/lovelock/storm-demo&#34;&gt;storm-demo&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;理论&#34;&gt;理论&lt;/h2&gt;

&lt;h3 id=&#34;概述&#34;&gt;概述&lt;/h3&gt;

&lt;p&gt;Apache Storm是一个自由并且开源的&lt;strong&gt;分布式实时&lt;/strong&gt;计算系统.它使得像Hadoop做批处理一样做&lt;strong&gt;实时的&lt;/strong&gt;、&lt;strong&gt;无限量&lt;/strong&gt;的&lt;strong&gt;流数据&lt;/strong&gt;处理变得简单可靠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://storm.apache.org/images/storm-flow.png&#34; alt=&#34;Apache Storm工作流程&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;概念解释&#34;&gt;概念解释&lt;/h3&gt;

&lt;h4 id=&#34;工作原理&#34;&gt;工作原理&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://www.tutorialspoint.com/apache_storm/images/zookeeper_framework.jpg&#34; alt=&#34;Apache Storm组件间关系&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Nimbus&lt;br /&gt;
 Nimbus是Storm集群的&lt;strong&gt;主节点master node&lt;/strong&gt;。Storm集群中除Nimbus节点之外的所有节点叫做&lt;strong&gt;工作节点worker nodes&lt;/strong&gt;。&lt;br /&gt;
 Nimbus负责三项工作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;向worker nodes分发数据&lt;/li&gt;
&lt;li&gt;向worker nodes分配tasks&lt;/li&gt;
&lt;li&gt;监控失败&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Supervisor&lt;br /&gt;
 接受Nimbus的指令的节点叫做Supervisors（监工），它有&lt;strong&gt;多个worker process&lt;/strong&gt;，并控制worker process完成Nimbus分配的tasks&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Worker Process&lt;br /&gt;
 Worker process执行指定Topology的tasks。&lt;strong&gt;worker process自己并不实际执行tasks，而是创建executors并由executors执行指定的task。&lt;/strong&gt;一个worker process可以由多个executor。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Executor&lt;br /&gt;
 Executor是由worker process创建的线程。一个executor执行一个或多个tasks，但只为&lt;strong&gt;一个指定的Spout或者Bolt工作&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Task&lt;br /&gt;
 Task是实际的数据处理工作，所以它可能是一个Spout或者Bolt。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;配套服务&#34;&gt;配套服务&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;ZooKeeper&lt;br /&gt;
&lt;a href=&#34;http://zookeeper.apache.org/&#34;&gt;ZooKeeper&lt;/a&gt;是一个分布式的配置分发服务。Storm和Kafka都是无状态的，它们的工作需要外部服务为其维持状态，如Storm从Kafka中取数据时需要的partition编号和offset偏移量等诸如此类的信息。ZooKeeper会综合分析Spout和Bolt发送来的ack或者fail请求来决定是否更新offset。如下图所示&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;http://ww3.sinaimg.cn/large/65e4f1e6jw1f8wd8tm94ij21kw097q5h.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Kafka&lt;br /&gt;
&lt;a href=&#34;http://kafka.apache.org/&#34;&gt;Kafka&lt;/a&gt;是一个分布式的消息系统。支持&lt;strong&gt;点对点&lt;/strong&gt;和&lt;strong&gt;发布-订阅&lt;/strong&gt;两种消息模式。在和Storm配合中，充当&lt;strong&gt;数据来源&lt;/strong&gt;的角色。用&lt;a href=&#34;https://github.com/apache/storm/tree/master/external/storm-kafka&#34;&gt;KafkaSpout&lt;/a&gt;和Storm进行组合。&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;本文只关注Storm，有关ZooKeeper和Kafka的介绍，可以访问&lt;a href=&#34;http://apache.org/&#34;&gt;官网&lt;/a&gt;、&lt;a href=&#34;http://www.tutorialspoint.com/&#34;&gt;TutorialsPoint&lt;/a&gt;或本博客的其他相关文章。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;拓扑作业&#34;&gt;拓扑作业&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Tuple&lt;br /&gt;
 Tuple是Topology中数据流的传输格式。它是&lt;strong&gt;不可变的键值对组&lt;/strong&gt;。既然是键值对，就需要设置键和值，典型的设置方式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt; // 设置键
 outputFieldsDeclarer.declare(new Fields(&amp;quot;timestamp&amp;quot;, &amp;quot;fieldvalues&amp;quot;));
 // 设置值
 collector.emit(tuple, new Values(timestamp, stringBuilder.toString()));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就会得到一个形如&lt;code&gt;(&amp;quot;timestamp&amp;quot;: timestamp, &amp;quot;fieldvalues&amp;quot;: xxxx&amp;quot;)&lt;/code&gt;这样的Tuple。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Spout&lt;br /&gt;
 Spout是Topology的数据来源，输出的数据以Tuple的形式传入下一个Bolt。具体到本例中，KafkaSpout会把它接收到的数据以类似&lt;code&gt;(0: message)&lt;/code&gt;这样的形式发射(emit)出来。所以，在KafkaSpout下游的Bolt需要这样获取整条数据(其实这里是可配置的)：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt; String message = tuple.getString(0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对KafkaSpout而言，它也实现了多个方法，但我们这里只需要了解两个&lt;code&gt;ack&lt;/code&gt;和&lt;code&gt;fail&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ww3.sinaimg.cn/large/65e4f1e6jw1f8wdqv4tuqj218y0ji41l.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这两个是回调方法，分别在ack unit向其发送ack或fail请求时被触发，一般而言，ack方法由于通知Kafka发送下一条数据，fail方法用于通知Kafka重发上一条数据。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;关于ack unit，可以理解为Storm中处理bolt的ack或fail请求的总线，它接收到相应的请求后，做简单的异或处理，并把结果发送给Spout。&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bolt&lt;br /&gt;
 Bolt是真正写处理逻辑的地方，比如在本例中，我们要做以下几件事：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;把message中的每个字段提取出来，&lt;/li&gt;
&lt;li&gt;从message的domain字段中过滤出以&lt;code&gt;.api.ksyun.com&lt;/code&gt;结尾的，其他的舍弃&lt;/li&gt;
&lt;li&gt;把domain字段的值以&lt;code&gt;.&lt;/code&gt;分割，取出index为0的部分，也就是第一段作为service字段&lt;/li&gt;
&lt;li&gt;把service最终输出Tuple的一个field写入输出结果&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;一般情况下，要实现一个Bolt有几种方式&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;实现&lt;code&gt;IRichBolt&lt;/code&gt;接口&lt;br /&gt;
  因为这个比较低级，要实现的方法有很多，而其中多数的方法不需要做特殊处理，所以一般会用第二种方式&lt;/li&gt;

&lt;li&gt;&lt;p&gt;集成&lt;code&gt;BaseRichBolt&lt;/code&gt;类&lt;br /&gt;
  这个基类实现了&lt;code&gt;IRichBolt&lt;/code&gt;中定义的几个不常用的方法，让我们只需要关注重点的几个方法即可。在这种方式中，我们需要自己实现三个方法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector)&lt;/code&gt;&lt;br /&gt;
  这个方法&lt;strong&gt;类似构造函数&lt;/strong&gt;，用来做一些准备工作，通常用于&lt;strong&gt;把上游传来的collector赋值给成员变量&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;public void execute(Tuple tuple)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这是最核心的方法。它负责：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;从上游传来的Tuple中读取感兴趣的字段&lt;/li&gt;
&lt;li&gt;把这些字段做一些处理后产生一组新的字段&lt;/li&gt;
&lt;li&gt;把这些值通过&lt;code&gt;OutputCollector::emit(new Values())&lt;/code&gt;方法发射出去&lt;/li&gt;
&lt;li&gt;向上游发送&lt;code&gt;OutputCollector::ack(Tuple tuple)&lt;/code&gt;或&lt;code&gt;OutputCollector::fail(Tuple tuple)&lt;/code&gt;，以告知上游本次Tuple处理是否成功。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;前面已经说过，Tuple是数据交流的格式，这个方法就是用来定义发送到下游的Tuple的字段名的。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Topology&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://storm.apache.org/releases/0.10.0/images/topology.png&#34; alt=&#34;一个典型的Topology&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上面这张图中有4个Topology，说明了几个问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一个Spout就是一个Topology的入口，从Spout分出几条线就有几个Topology&lt;/li&gt;
&lt;li&gt;一个Topology由一个Spout和若干个Bolt组成&lt;/li&gt;
&lt;li&gt;Topology之间可以共享Spout或者Bolt&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;创建一个Topology的典型过程：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;    TopologyBuilder topologyBuilder = new TopologyBuilder();
    topologyBuilder.setSpout(KAFKA_SPOUT_ID, kafkaSpout, 10);
    topologyBuilder.setBolt(CROP_BOLT_ID, new CropBolt(), 10).shuffleGrouping(KAFKA_SPOUT_ID);
    topologyBuilder.setBolt(SPLIT_FIELDS_BOLT_ID, new SplitFieldsBolt(), 10).shuffleGrouping(CROP_BOLT_ID);
    topologyBuilder.setBolt(STORM_HDFS_BOLT_ID, hdfsBolt, 10).fieldsGrouping(SPLIT_FIELDS_BOLT_ID, new Fields(&amp;quot;timestamp&amp;quot;, &amp;quot;fieldvalues&amp;quot;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的代码可以看到，&lt;code&gt;TopologyBuilder&lt;/code&gt;类通过&lt;code&gt;setSpout()&lt;/code&gt;和&lt;code&gt;setBolt()&lt;/code&gt;两个方法生动的反映了上图的工作流程。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;源码分析&#34;&gt;源码分析&lt;/h2&gt;

&lt;h3 id=&#34;cropbolt-java&#34;&gt;&lt;code&gt;CropBolt.java&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;主要关注&lt;code&gt;execute&lt;/code&gt;方法，它首先从上游发射来的Tuple中取出第一个字段，也就是整条消息作为一个字符串。根据对字符串的分析，我们知道该字符串是以&lt;code&gt;\t\t&lt;/code&gt;作为字段间分隔符，以&lt;code&gt;:&lt;/code&gt;作为键值分隔符的字符串，所以可以写一个方法来用这种规则解析出消息中的所有字段，并把它放在一个HashMap里。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;    private HashMap makeMapOfMessage(String message) {
    String[] fields = message.split(ServerConfig.getFieldSeparator());
    HashMap&amp;lt;String, String&amp;gt; map = new HashMap&amp;lt;&amp;gt;();

    try {
        for (String field : fields) {
            String[] pair = field.split(ServerConfig.getPairSeparator(), 2);
            map.put(pair[0], pair[1]);
        }
    } catch (ArrayIndexOutOfBoundsException e) {
        LOG.warn(&amp;quot;makeMapOfMessage failed {}&amp;quot;, message);
        e.printStackTrace();
    }

    return map;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中&lt;code&gt;ServerConfig&lt;/code&gt;是一个工具类，它提供了简单的API，把处理逻辑和配置信息分离。在本例中我用的分隔符和实际项目并不一样，这个差别只需要在相应的properties配置文件中做修改即可。还需要注意异常处理，这个方法的返回值有可能是null，在调用该方法的地方需要做相应的判断。&lt;/p&gt;

&lt;p&gt;在&lt;code&gt;execute&lt;/code&gt;方法中，从上述方法的返回值取出关心的字段，并按需求解析出需要的&lt;code&gt;service&lt;/code&gt;字段，并通过&lt;code&gt;collector.emit&lt;/code&gt;发送给下游的Bolt。&lt;/p&gt;

&lt;p&gt;这个方法中有三点需要注意：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;没有在&lt;code&gt;try&lt;/code&gt;子句中调用&lt;code&gt;ack&lt;/code&gt;方法&lt;/li&gt;
&lt;li&gt;没有在&lt;code&gt;catch&lt;/code&gt;子句中调用&lt;code&gt;fail&lt;/code&gt;方法&lt;/li&gt;
&lt;li&gt;在&lt;code&gt;finally&lt;/code&gt;子句中调用了&lt;code&gt;ack&lt;/code&gt;方法&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;因为我们catch住的这种情况，是只有在输入的数据不满足我们约定要求的情况下才会发生的，比如某些必要的字段不存在等，而这种情况在当前的Topology中是不需要处理的，并且也不需要重试，因此，不需要调用&lt;code&gt;fail&lt;/code&gt;。同时，不管数据是否符合要求，我们都是需要通知Spout&lt;strong&gt;这里的处理已经完成&lt;/strong&gt;这个信息的，所以在&lt;code&gt;finally&lt;/code&gt;中调用&lt;code&gt;ack&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;splitfieldsbolt-java&#34;&gt;&lt;code&gt;SplitFieldsBolt.java&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;这步的功能很简单，就是把前面传过来的所有字段用一个特定的分隔符连接起来，变成一行数据。只有一个特殊，也就是&lt;code&gt;service&lt;/code&gt;字段，它不是直接取出来的，而是前面的Bolt通过一些处理得到的，所以这是&lt;code&gt;stringBuilder&lt;/code&gt;需要处理的一种特殊情况。&lt;/p&gt;

&lt;p&gt;最后把『时间戳』和『各个字段的值』发射给下游的&lt;code&gt;HdfsBolt&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;hdfsbolt&#34;&gt;&lt;code&gt;HdfsBolt&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;HdfsBolt&lt;/code&gt;是Storm到HDFS的一个中转层，配置一些规则，把Storm输出的数据写入HDFS。其中比较重要的配置有：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;DelimitedRecordFormat&lt;/code&gt; 要写入的字段 在本例中，『时间戳』只是用来划分目录的，所以不需要写入HDFS中&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CountSyncPolicy&lt;/code&gt; 指定当内存中超过多少条数据时cache到磁盘中&lt;/li&gt;
&lt;li&gt;&lt;code&gt;FileSizeRotationPolicy&lt;/code&gt; 指定cache的文件超过多大时将文件写入文件系统，如果该值设置的较大，而数据流量又不太大的情况下，文件通常不会达到设置的值，因为当等待写入的文件未达到限制大小而先达到超时时间时，也会创建一个新的文件。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DefaultFileNameFormat&lt;/code&gt; 指定文件写入HDFS中的根目录和文件后缀&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Partitioner&lt;/code&gt; 指定分块规则，在本例中，我们根据日志中&lt;code&gt;time_local&lt;/code&gt;字段划分相应的消息应该写入的HDFS目录，比如&lt;code&gt;31/Aug/2016:13:08:12 +0800&lt;/code&gt;，相应的记录就会写入&lt;code&gt;root/20160831/13&lt;/code&gt;目录中。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;FsURL&lt;/code&gt; 当然需要指定正确的&lt;code&gt;HDFS&lt;/code&gt;服务。&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;其实我们的KMR对应的Storm 0.10.0是不支持HDFS 的partition的，这里我是把Storm最新版的2.0.0-SNAPSHOT中相应的代码移植过来用的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;logstatisticstopology-java&#34;&gt;&lt;code&gt;LogStatisticsTopology.java&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;前面讲过，这是拓扑作业的入口，这里指定了一条消息要通过的路径。需要注意的有以下几点：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;setSpout&lt;/code&gt;和&lt;code&gt;setBolt&lt;/code&gt;方法中的parallelism_hint(并行度建议)，前面说了，Spout和Bolt在Storm中是以executor的形式存在的，而这个值就是指定executor的数量。但又没有那么绝对，比如在KafkaSpout中，如果指定的Topic在Kafka中有10个partition，但这里的KafkaSpout指定了15个并行度，实际还是只有10个executor有意义，因为剩余的5个在前面10个都正常工作的情况下是分配不到任何数据的，由于ZooKeeper做了中间人，它是知道每个Topic有多少个partition的，所以这里设置多于partition数量的并行度也是不起作用的。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;grouping类型 Storm目前支持4种分组形式。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;随机分组 等量tuples随机分发给执行bolt的所有workers&lt;/li&gt;
&lt;li&gt;字段分组 把指定字段值相同的分配给同一个task,在wordCount应用中比较重要&lt;/li&gt;
&lt;li&gt;广播分组 给每个executor发送一个这个tuple的副本&lt;/li&gt;
&lt;li&gt;全局分组 把所有数据分发给bolt的executor中id最小的&lt;strong&gt;一个&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;无分组   目前基本等同于随机分组，会把tuple交给和它上游同一个线程内的下游bolt，以减少数据传递的开销&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;根据我们的需求，其实是不需要太关心分组的事情。&lt;/p&gt;

&lt;p&gt;关于优化，有几个方面可以考虑：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;考虑到后面需要用Hive分析数据，如果产生很多小的文件，就会产生过多的Map过程，影响性能，可以考虑同一小时的文件交给同一个executor来写，因为每个executor会打开一个hdfs文件，但这样可能会导致并发数过少&lt;/li&gt;
&lt;li&gt;既然这样，可以减少executor的数量，比如现在是10个，可以改成5个，在不触发FileSizeRotationPolicy的情况下，把生成的文件数量减少了一半，也就把Hive查询时Map过程的数量减少了一半&lt;/li&gt;
&lt;li&gt;分析需求，如果没有按照小时分组的需求，可以直接删除这个级别，直接用天作为区分，这样，在不触发FileSizeRotationPolicy的情况下产生的文件数量会变成1/24,相应的Hive查询中Map的过程也会变成1/24.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;本文主要介绍了Storm的工作流程，以及其与Kafka和HDFS的配合来进行日志分析的工作流程，并简单介绍了一些需要注意的点。&lt;/p&gt;

&lt;p&gt;我也是Storm的初学者，并没有太多的实际经验，这里只是演示了一个可以工作的Storm作业的实例，大家有什么问题可以及时与我交流，也希望大家不吝赐教。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
